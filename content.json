{"pages":[],"posts":[{"title":"Matplotlibnç¬”è®°ğŸ“’","text":"12import matplotlib.pyplot as pltimport numpy as np 123x = np.linspace(1,10)y1 = 2*x + 1y2 = x**2 12345678910111213141516171819202122232425262728293031plt.figure(num = 3, figsize=(8,5))# x,yçš„æ ‡é¢˜plt.xlabel('$x$');plt.ylabel('$y$')plt.yticks([0,50,100], ['$bad$','$normal$','$good$'])# gca = get current axisax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.yaxis.set_ticks_position('left')# ax.spines['bottom'].set_position(('data',0))# ax.spines['left'].set_position(('data',0))l1, = plt.plot(x,y2,label = '$up$')l2, = plt.plot(x,y1,color='red',linewidth=1.0,linestyle='--',label='$down$')plt.legend(handles=[l1,l2],loc='best')# æ³¨è§£x0 = 6y0 = x0**2plt.scatter(x0,y0,s=20) # å±•ç¤ºç‚¹plt.plot([x0,x0],[y0,0],'k--',lw=1)plt.annotate(r'$2x+1=%s$' % y0,xy=(x0,y0),xycoords='data',xytext=(8,30), arrowprops=dict(arrowstyle='-&gt;',connectionstyle='arc3,rad=.2'),color='pink')plt.text(2,50,r'$This\\ is\\ some\\ text.\\ \\mu\\ \\sigma_i\\ \\alpha_t$',fontdict={'size':12,'color':'red'})plt.show() 1234def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2)def z(x,y): return (x**2 + y**2) 123456789101112n = 256x = np.linspace(-3,3,n)y = np.linspace(-3,3,n)X,Y = np.meshgrid(x,y)plt.contourf(X,Y,f(X,Y),7,alpha=0.5,cmap=plt.cm.cool)C = plt.contour(X,Y,f(X,Y),7,colors='black')plt.clabel(C,inline=True,)plt.xticks(())plt.yticks(())plt.show() 1234a = np.random.normal(0,1,100).reshape((10,10))plt.imshow(a,interpolation='nearest')plt.colorbar(shrink=0.9)plt.show &lt;function matplotlib.pyplot.show(*args, **kw)&gt; 1from mpl_toolkits.mplot3d import Axes3D 12345678910fig = plt.figure()ax = Axes3D(fig)X = np.arange(-4,4,0.25)Y = np.arange(-4,4,0.25)X,Y = np.meshgrid(X,Y)R = np.sqrt(X**2+Y**2)ax.plot_surface(X,Y,R,cmap='rainbow')ax.contourf(X,Y,R,zdir='z',offset=0,cmap='rainbow')plt.show() 12345678910plt.figure()plt.subplot(2,3,4)plt.plot(np.arange(1,100))plt.subplot(2,3,5)plt.plot(np.arange(1,20)**2)plt.subplot(2,1,1)plt.plot(np.random.normal(0,1,100))plt.subplot(2,3,6)plt.plot(np.array(np.sin(np.arange(0,10,0.1))))plt.show() 1import matplotlib.gridspec as gridspec 1234567891011# plt.figure()# ax1 = plt.subplot2grid((3,3),(0,0),colspan=3,rowspan=1)# ax1.plot(np.random.normal(0,1,100))# ax1.set_title('time')# ax2 = plt.subplot2grid((3,3),(1,0),colspan=2,rowspan=1)# ax2 = plt.subplot2grid((3,3),(1,2),colspan=1,rowspan=2)# ax2 = plt.subplot2grid((3,3),(2,0),colspan=1,rowspan=1)# ax2 = plt.subplot2grid((3,3),(2,1),colspan=1,rowspan=1)# plt.show() 1234567# plt.figure()# gs = gridspec.GridSpec(3,3)# ax1 = plt.subplot(gs[0,:])# ax2 = plt.subplot(gs[1,:2])# ax3 = plt.subplot(gs[1:,2])# ax4 = plt.subplot(gs[-1,0])# ax5 = plt.subplot(gs[-1,1]) 123456789101112x = np.arange(0,50)y1 = np.random.normal(0,1,50)y2 = np.random.normal(1,2,50)fig,ax1 = plt.subplots()ax2 = ax1.twinx()ax1.plot(x,y1,'g-')ax2.plot(x,-y2,'b-')ax1.set_xlabel('X data')ax1.set_ylabel('Y1',color='g')ax2.set_ylabel('Y2',color='b')plt.show() 1from scipy.stats import t 12345x = np.arange(-5,5,0.1)plt.figure()plt.plot(x,t.pdf(x,2))plt.plot(x,t.pdf(x,10))plt.show() 12","link":"/2019/03/26/matplotlib/"},{"title":"æˆ‘çš„ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œâ€”â€”æ‰‹å†™æ•°å­—è¯†åˆ«","text":"å¯ä»è¯¥é¡µé¢è·å¾—çš„MNISTæ‰‹å†™æ•°å­—æ•°æ®åº“å…·æœ‰60,000ä¸ªç¤ºä¾‹çš„è®­ç»ƒé›†å’Œ10,000ä¸ªç¤ºä¾‹çš„æµ‹è¯•é›†ã€‚å®ƒæ˜¯NISTæä¾›çš„æ›´å¤§é›†åˆçš„å­é›†ã€‚æ•°å­—å·²ç»è¿‡å°ºå¯¸æ ‡å‡†åŒ–ï¼Œå¹¶ä»¥å›ºå®šå°ºå¯¸çš„å›¾åƒä¸ºä¸­å¿ƒã€‚ æ‰‹å†™æ•°å­—æ•°æ®é›†THE MNIST DATABASE 123456789101112131415161718192021222324from keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.optimizers import Adamfrom keras.models import Sequentialfrom keras.layers import Dense, Activation# æ¸…æ´—æ•°æ®def load_data(): global x_train, x_test, y_train, y_test x_train = x_train.reshape(x_train.shape[0], 28*28) x_test = x_test.reshape(x_test.shape[0], 28*28) x_train = x_train.astype('float32') x_test = x_test.astype('float32') y_train = np_utils.to_categorical(y_train, 10) y_test = np_utils.to_categorical(y_test, 10) x_train = x_train/255 x_test = x_test/255 return (x_train, y_train),(x_test, y_test)# å¯¼å…¥æ•°æ®(x_train, y_train),(x_test, y_test) = mnist.load_data()(x_train, y_train),(x_test, y_test) = load_data() 12345678# æ¨¡å‹æ­å»ºmodel = Sequential()model.add(Dense(input_dim=28*28, units=500))model.add(Activation('relu'))model.add(Dense(units=500))model.add(Activation('relu'))model.add(Dense(units=10))model.add(Activation('softmax')) 1234# æ¨¡å‹é…ç½®model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) 1234# è®­ç»ƒmodel.fit(x_train, y_train, epochs=20, batch_size=100) Instructions for updating: Use tf.cast instead. Epoch 1/20 60000/60000 [==============================] - 5s 89us/step - loss: 0.2038 - acc: 0.9390 ... 60000/60000 [==============================] - 5s 86us/step - loss: 0.0051 - acc: 0.9985 Epoch 20/20 60000/60000 [==============================] - 5s 84us/step - loss: 0.0095 - acc: 0.9973 &lt;keras.callbacks.History at 0x103d9b710&gt; 1234# è®­ç»ƒè¯„åˆ†score = model.evaluate(x_train, y_train, batch_size=100)print('Total loss in training set:', score[0])print('Accuracy in training set:', score[1]) 60000/60000 [==============================] - 2s 26us/step Total loss in training set: 0.030331732142293125 Accuracy in training set: 0.9930500061313311 1234# æµ‹è¯•è¯„åˆ†score = model.evaluate(x_test, y_test, batch_size=100)print('Total loss in testing set:', score[0])print('Accuracy in testing set:', score[1]) 10000/10000 [==============================] - 0s 27us/step Total loss in testing set: 0.13895348183807527 Accuracy in testing set: 0.9754000073671341","link":"/2019/04/02/MNIST/"},{"title":"ç•™è¨€æ¿ ğŸ“®","text":"æ¬¢è¿ç•™ä¸‹è¯„è®ºï½","link":"/2000/03/02/ç•™è¨€æ¿/"},{"title":"ä¸€é“æ’åºé¢˜","text":"ç»™å®šä¸€ç»„æ•°ï¼Œæ¯æ¬¡åªèƒ½äº¤æ¢ä¸¤ä¸ªæ•°å­—çš„ä½ç½®ï¼Œæ±‚å…¨éƒ¨æ’åºå®Œæˆçš„æœ€å°äº¤æ¢æ¬¡æ•° 12345678910111213141516def s(l): k = 0 for i in range(len(l)): li = l[i:] j = li.index(min(li)) if j == 0: continue l[i], l[i+j] = l[i+j], l[i] k += 1 return kl = [5,4,3,2,1]print(s(l))# &gt;&gt;&gt; 2","link":"/2019/03/30/ä¸€é“æ’åºé¢˜/"},{"title":"ç¡¬trainçš„æ•…äº‹","text":"Thanks a lot, machine learning! Joel GrusFizz Buzz in Tensorflow","link":"/2019/03/30/video/"},{"title":"Numpyç¬”è®°ğŸ“’","text":"NumPy(Numerical Python) æ˜¯ Python è¯­è¨€çš„ä¸€ä¸ªæ‰©å±•ç¨‹åºåº“ï¼Œæ”¯æŒå¤§é‡çš„ç»´åº¦æ•°ç»„ä¸çŸ©é˜µè¿ç®—ï¼Œæ­¤å¤–ä¹Ÿé’ˆå¯¹æ•°ç»„è¿ç®—æä¾›å¤§é‡çš„æ•°å­¦å‡½æ•°åº“ã€‚123456import numpy as npl = [[1,2,3],[2,3,4]]# åˆ—è¡¨è½¬åŒ–æˆçŸ©é˜µarray = np.array(l)print(array) [[1 2 3] [2 3 4]] numpyçš„å‡ ç§å±æ€§123456# ç»´åº¦print('number of dim:',array.ndim)# è¡Œæ•°å’Œåˆ—æ•°print('shape :',array.shape) # å…ƒç´ ä¸ªæ•°print('size:',array.size) number of dim: 2 shape : (2, 3) size: 6 åˆ›å»ºarray12a = np.array([2,3,4],dtype = np.int)print(a,a.dtype) [2 3 4] int64 123# åˆ›å»ºå…¨0æ•°ç»„a = np.zeros((3,4),dtype = np.int16)print(a) [[0 0 0 0] [0 0 0 0] [0 0 0 0]] 123# åˆ›å»ºå…¨1æ•°ç»„a = np.ones((3,4),dtype = np.int64)print(a) [[1 1 1 1] [1 1 1 1] [1 1 1 1]] 123# åˆ›å»ºå…¨ç©ºæ•°ç»„a = np.empty((3,4),dtype = np.float)print(a) [[-1.49166815e-154 -1.49166815e-154 4.27255699e+180 6.12033286e+257] [ 3.83819517e+151 9.77368093e+165 1.03927302e-042 5.24049485e+174] [ 4.27796595e-033 5.81088333e+294 -1.49166815e-154 8.38743761e-309]] 123# åˆ›å»ºè¿ç»­æ•°ç»„a = np.arange(1,13).reshape(3,4)print(a) [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] 123# ç”Ÿæˆçº¿æ®µa = np.linspace(1,10,6).reshape(2,3)print(a) [[ 1. 2.8 4.6] [ 6.4 8.2 10. ]] åŸºç¡€è¿ç®—åŸºæœ¬è¿ç®—1æ•°ç»„è¿ç®—12a = np.array([10,20,30,40])b = np.arange(4) 1a-b array([10, 19, 28, 37]) 1b**2 array([0, 1, 4, 9]) 1np.sin(a) array([-0.54402111, 0.91294525, -0.98803162, 0.74511316]) 1b&lt;3 array([ True, True, True, False]) çŸ©é˜µè¿ç®—1234a = np.array([[1,2],[3,4]])b = np.arange(4).reshape(2,2)print(a)print(b) [[1 2] [3 4]] [[0 1] [2 3]] 1234# å¯¹åº”å…ƒç´ ç›¸ä¹˜print(a*b)# çŸ©é˜µä¹˜æ³•print(np.dot(a,b)) [[ 0 2] [ 6 12]] [[ 4 7] [ 8 15]] 123456a = np.random.random((2,4))print(a)# axis=1æ—¶æ¯è¡Œåˆ†åˆ«è®¡ç®—ï¼Œaxis=0æ—¶æ¯åˆ—åˆ†åˆ«è®¡ç®—print('sum =',np.sum(a,axis=1))print('min =',np.min(a,axis=0))print('max =',np.max(a)) [[0.31357817 0.09926399 0.57284534 0.9692283 ] [0.86206853 0.94729865 0.80886452 0.01849844]] sum = [1.9549158 2.63673014] min = [0.31357817 0.09926399 0.57284534 0.01849844] max = 0.9692282997410551 åŸºæœ¬è¿ç®—212A = np.arange(2,14).reshape(3,4)print(A) [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] 12# æœ€å°ï¼ˆå¤§ï¼‰å€¼ç´¢å¼•A.argmax() 11 1234# å¹³å‡å€¼print(A.mean())# å¯¹åˆ—æ±‚å¹³å‡print(A.mean(axis=0)) 7.5 [6. 7. 8. 9.] 12# ä¸­ä½æ•°np.median(A) 7.5 12# ç´¯åŠ print(np.cumsum(A)) [ 2 5 9 14 20 27 35 44 54 65 77 90] 12# ç´¯å·®print(np.diff(A)) [[1 1 1] [1 1 1] [1 1 1]] 12# æ’åºprint(np.sort(A)) [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] 12# è½¬ç½®print(np.transpose(A)) [[ 2 6 10] [ 3 7 11] [ 4 8 12] [ 5 9 13]] 12# A*ATprint(np.dot(A,A.T)) [[ 54 110 166] [110 230 350] [166 350 534]] 12# æ»¤æ³¢print(np.clip(A,5,9)) [[5 5 5 5] [6 7 8 9] [9 9 9 9]] 12# é“ºå¹³print(A.flatten()) [ 2 3 4 5 6 7 8 9 10 11 12 13] numpyç´¢å¼•12A = np.arange(3,15).reshape(3,4)print(A) [[ 3 4 5 6] [ 7 8 9 10] [11 12 13 14]] 12A[1,1]#A[1][1] 8 arrayåˆå¹¶1234A = np.array([1,1,1])B = np.array([2,2,2])np.vstack((A,B)) array([[1, 1, 1], [2, 2, 2]]) 1np.hstack((A,B)) array([1, 1, 1, 2, 2, 2]) arrayåˆ†å‰²12A = np.arange(12).reshape(3,4)print(A) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 12345np.split(A,3)# æ¨ªå‘åˆ†å‰²# np.vsplit(A,3)# çºµå‘åˆ†å‰²# np.hsplit(A,2) [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] 12# ä¸ç­‰é‡åˆ†å‰²np.array_split(A,3,axis=1) [array([[0, 1], [4, 5], [8, 9]]), array([[ 2], [ 6], [10]]), array([[ 3], [ 7], [11]])] copy123a = np.array([1,2])# b.copy()æ²¡æœ‰å…³è”æ€§ï¼Œä¸ºæµ…æ‹·è´a is b.copy() False","link":"/2019/03/02/numpy/"},{"title":"Kerasæ‰‹æŠŠæ‰‹æ•™å­¦ä¹‹ ğŸ’¯","text":"Keras Sequential é¡ºåºæ¨¡å‹ æ¨¡å‹æ­å»ºç›´æ¥çœ‹ä¾‹å­ç‚¹è¿™é‡Œ ğŸ‘ˆğŸ» Sequential æ¨¡å‹æ˜¯å±‚çš„çº¿æ€§å †æ ˆ 12345678910111213141516import kerasfrom keras.models import Sequentialfrom keras.layers import Dense, Activationmodel = Sequential()# Dense: fully connection(DNNå¯†é›†å‹ç¥ç»ç½‘ç»œ)model.add(Dense(input_dim=28*28,units=500))# activation function:# softplus, softsign, relu, tanh, hard_sigmoid, linearmodel.add(Activation('sigmoid'))# the second hidden layermodel.add(Dense(units=500))model.add(Activation('sigmoid'))model.add(Dense(units=10))model.add(Activation('softmax')) æ¨¡å‹é…ç½®åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦é…ç½®å­¦ä¹ è¿‡ç¨‹ï¼Œè¿™æ˜¯é€šè¿‡ compile æ–¹æ³•å®Œæˆçš„ã€‚å®ƒæ¥æ”¶ä¸‰ä¸ªå‚æ•°ï¼š ä¼˜åŒ–å™¨ optimizerã€‚å®ƒå¯ä»¥æ˜¯ç°æœ‰ä¼˜åŒ–å™¨çš„å­—ç¬¦ä¸²æ ‡è¯†ç¬¦ï¼Œå¦‚ rmsprop æˆ– adagradï¼Œä¹Ÿå¯ä»¥æ˜¯ Optimizer ç±»çš„å®ä¾‹ã€‚ æŸå¤±å‡½æ•° lossï¼Œæ¨¡å‹è¯•å›¾æœ€å°åŒ–çš„ç›®æ ‡å‡½æ•°ã€‚å®ƒå¯ä»¥æ˜¯ç°æœ‰æŸå¤±å‡½æ•°çš„å­—ç¬¦ä¸²æ ‡è¯†ç¬¦ï¼Œå¦‚ categorical_crossentropy æˆ– mseï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªç›®æ ‡å‡½æ•°ã€‚ è¯„ä¼°æ ‡å‡† metricsã€‚å¯¹äºä»»ä½•åˆ†ç±»é—®é¢˜ï¼Œä½ éƒ½å¸Œæœ›å°†å…¶è®¾ç½®ä¸º metrics = ['accuracy']ã€‚ 123456789101112131415161718# äºŒåˆ†ç±»é—®é¢˜model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])# å‡æ–¹è¯¯å·®å›å½’é—®é¢˜model.compile(optimizer='rmsprop', loss='mse')# è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†å‡½æ•°import keras.backend as Kdef mean_pred(y_true, y_pred): return K.mean(y_pred)model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred]) 123456# step 3.1:# Configuration# optimizer = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) æ¨¡å‹è®­ç»ƒKeras æ¨¡å‹åœ¨è¾“å…¥æ•°æ®å’Œæ ‡ç­¾çš„ Numpy çŸ©é˜µä¸Šè¿›è¡Œè®­ç»ƒã€‚ä¸ºäº†è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œä½ é€šå¸¸ä¼šä½¿ç”¨ fit å‡½æ•°ã€‚ 1234567# å…·æœ‰ 10 ä¸ªç±»çš„å•è¾“å…¥æ¨¡å‹ï¼ˆå¤šåˆ†ç±»åˆ†ç±»ï¼‰ï¼š# ç”Ÿæˆè™šæ‹Ÿæ•°æ®import numpy as npdata = np.random.random((50000, 28*28))labels = np.random.randint(10, size=(50000, 1))# å°†æ ‡ç­¾è½¬æ¢ä¸ºåˆ†ç±»çš„ one-hot ç¼–ç one_hot_labels = keras.utils.to_categorical(labels, num_classes=10) 1234567891011# step 3.2:# Find the optimal network parameters# do not really minimize total loss# betch_size:# --&gt; pick the 1st batch L' = L1 + L31 + ...# update parameters once# --&gt; pick the 2nd batch ...# all of the betch make one epoch: need 20 epoch# when beach_size = 1:# Stochastic gradient descentmodel.fit(data, one_hot_labels, epochs=20, batch_size=100) Epoch 1/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.3052 - acc: 0.1004 Epoch 2/20 50000/50000 [==============================] - 5s 92us/step - loss: 2.3046 - acc: 0.1019 Epoch 3/20 50000/50000 [==============================] - 4s 87us/step - loss: 2.3053 - acc: 0.0997 Epoch 4/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.3057 - acc: 0.1014 Epoch 5/20 50000/50000 [==============================] - 5s 92us/step - loss: 2.3057 - acc: 0.1027 Epoch 6/20 50000/50000 [==============================] - 5s 91us/step - loss: 2.3065 - acc: 0.1035 Epoch 7/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.3077 - acc: 0.1034 Epoch 8/20 50000/50000 [==============================] - 5s 94us/step - loss: 2.3081 - acc: 0.1039 Epoch 9/20 50000/50000 [==============================] - 4s 89us/step - loss: 2.3057 - acc: 0.1109 Epoch 10/20 50000/50000 [==============================] - 4s 87us/step - loss: 2.3012 - acc: 0.1190 Epoch 11/20 50000/50000 [==============================] - 4s 87us/step - loss: 2.2925 - acc: 0.1285 Epoch 12/20 50000/50000 [==============================] - 4s 88us/step - loss: 2.2836 - acc: 0.1375 Epoch 13/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.2744 - acc: 0.1476 Epoch 14/20 50000/50000 [==============================] - 4s 86us/step - loss: 2.2675 - acc: 0.1526 Epoch 15/20 50000/50000 [==============================] - 4s 85us/step - loss: 2.2638 - acc: 0.1551 Epoch 16/20 50000/50000 [==============================] - 4s 80us/step - loss: 2.2556 - acc: 0.1602 Epoch 17/20 50000/50000 [==============================] - 4s 83us/step - loss: 2.2525 - acc: 0.1638 Epoch 18/20 50000/50000 [==============================] - 5s 94us/step - loss: 2.2534 - acc: 0.1617 Epoch 19/20 50000/50000 [==============================] - 5s 93us/step - loss: 2.2462 - acc: 0.1681 Epoch 20/20 50000/50000 [==============================] - 4s 83us/step - loss: 2.2403 - acc: 0.1694 &lt;keras.callbacks.History at 0x126444278&gt; ExampleåŸºäºå¤šå±‚æ„ŸçŸ¥å™¨ (MLP) çš„ softmax å¤šåˆ†ç±»ï¼ˆåˆ†ç±»å™¨ï¼‰ï¼š 123456789101112131415import kerasfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Activationfrom keras.optimizers import SGD# ç”Ÿæˆè™šæ‹Ÿæ•°æ®import numpy as np# 1000è¡Œ*20åˆ— çš„åŒºé—´åœ¨[0.,1.)çš„æµ®ç‚¹æ•°x_train = np.random.random((1000, 20))# ç”Ÿæˆ 1000è¡Œ*1åˆ— çš„åŒºé—´åœ¨[0-10)çš„æ•´æ•°ï¼Œéšåè½¬åŒ–ä¸ºåˆ†ç±»çš„ one-hot ç¼–ç ï¼ˆ1000è¡Œ*10åˆ— çš„(0,1)çŸ©é˜µï¼‰y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)# 100è¡Œ*20åˆ— çš„æµ‹è¯•é›†x_test = np.random.random((100, 20))# 100è¡Œ*10åˆ— çš„æµ‹è¯•æ ‡ç­¾y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) 123456789model = Sequential()# Dense(64) æ˜¯ä¸€ä¸ªå…·æœ‰ 64 ä¸ªéšè—ç¥ç»å…ƒçš„å…¨è¿æ¥å±‚ã€‚# åœ¨ç¬¬ä¸€å±‚å¿…é¡»æŒ‡å®šæ‰€æœŸæœ›çš„è¾“å…¥æ•°æ®å°ºå¯¸ï¼š# åœ¨è¿™é‡Œï¼Œæ˜¯ä¸€ä¸ª 20 ç»´çš„å‘é‡ã€‚model.add(Dense(units=64, activation='relu', input_dim=20))model.add(Dropout(rate=0.5))model.add(Dense(units=64, activation='relu'))model.add(Dropout(rate=0.5))model.add(Dense(units=10, activation='softmax')) 123456# ä¼˜åŒ–å™¨sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)# æ¨¡å‹é…ç½®model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) 1234# æ¨¡å‹è®­ç»ƒmodel.fit(x_train, y_train, epochs=20, batch_size=128) Epoch 1/20 1000/1000 [==============================] - 0s 402us/step - loss: 2.4034 - acc: 0.1010 Epoch 2/20 1000/1000 [==============================] - 0s 19us/step - loss: 2.3808 - acc: 0.0920 Epoch 3/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3396 - acc: 0.1120 Epoch 4/20 1000/1000 [==============================] - 0s 23us/step - loss: 2.3249 - acc: 0.1190 Epoch 5/20 1000/1000 [==============================] - 0s 20us/step - loss: 2.3356 - acc: 0.0980 Epoch 6/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3233 - acc: 0.1210 Epoch 7/20 1000/1000 [==============================] - 0s 18us/step - loss: 2.3070 - acc: 0.1080 Epoch 8/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3190 - acc: 0.1030 Epoch 9/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3112 - acc: 0.0900 Epoch 10/20 1000/1000 [==============================] - 0s 20us/step - loss: 2.3076 - acc: 0.0960 Epoch 11/20 1000/1000 [==============================] - 0s 15us/step - loss: 2.3067 - acc: 0.1060 Epoch 12/20 1000/1000 [==============================] - 0s 16us/step - loss: 2.3058 - acc: 0.1160 Epoch 13/20 1000/1000 [==============================] - 0s 16us/step - loss: 2.2988 - acc: 0.1180 Epoch 14/20 1000/1000 [==============================] - 0s 17us/step - loss: 2.3008 - acc: 0.1240 Epoch 15/20 1000/1000 [==============================] - 0s 20us/step - loss: 2.3036 - acc: 0.1090 Epoch 16/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3042 - acc: 0.1060 Epoch 17/20 1000/1000 [==============================] - 0s 18us/step - loss: 2.3016 - acc: 0.1070 Epoch 18/20 1000/1000 [==============================] - 0s 25us/step - loss: 2.3054 - acc: 0.1390 Epoch 19/20 1000/1000 [==============================] - 0s 39us/step - loss: 2.2960 - acc: 0.1200 Epoch 20/20 1000/1000 [==============================] - 0s 34us/step - loss: 2.3021 - acc: 0.0940 &lt;keras.callbacks.History at 0x1842de6b38&gt; 12# æ¨¡å‹è¯„åˆ†score = model.evaluate(x_test, y_test, batch_size=128) 100/100 [==============================] - 0s 13us/step","link":"/2019/03/27/keras notebook/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"æœºå™¨å­¦ä¹ ","slug":"æœºå™¨å­¦ä¹ ","link":"/tags/æœºå™¨å­¦ä¹ /"},{"name":"ç®—æ³•","slug":"ç®—æ³•","link":"/tags/ç®—æ³•/"}],"categories":[{"name":"python","slug":"python","link":"/categories/python/"},{"name":"machine learning","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"ç®—æ³•","slug":"ç®—æ³•","link":"/categories/ç®—æ³•/"}]}
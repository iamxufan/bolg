{"pages":[],"posts":[{"title":"Matplotlibn笔记📒","text":"12import matplotlib.pyplot as pltimport numpy as np 123x = np.linspace(1,10)y1 = 2*x + 1y2 = x**2 12345678910111213141516171819202122232425262728293031plt.figure(num = 3, figsize=(8,5))# x,y的标题plt.xlabel('$x$');plt.ylabel('$y$')plt.yticks([0,50,100], ['$bad$','$normal$','$good$'])# gca = get current axisax = plt.gca()ax.spines['right'].set_color('none')ax.spines['top'].set_color('none')ax.xaxis.set_ticks_position('bottom')ax.yaxis.set_ticks_position('left')# ax.spines['bottom'].set_position(('data',0))# ax.spines['left'].set_position(('data',0))l1, = plt.plot(x,y2,label = '$up$')l2, = plt.plot(x,y1,color='red',linewidth=1.0,linestyle='--',label='$down$')plt.legend(handles=[l1,l2],loc='best')# 注解x0 = 6y0 = x0**2plt.scatter(x0,y0,s=20) # 展示点plt.plot([x0,x0],[y0,0],'k--',lw=1)plt.annotate(r'$2x+1=%s$' % y0,xy=(x0,y0),xycoords='data',xytext=(8,30), arrowprops=dict(arrowstyle='-&gt;',connectionstyle='arc3,rad=.2'),color='pink')plt.text(2,50,r'$This\\ is\\ some\\ text.\\ \\mu\\ \\sigma_i\\ \\alpha_t$',fontdict={'size':12,'color':'red'})plt.show() 1234def f(x,y): return (1-x/2+x**5+y**3)*np.exp(-x**2-y**2)def z(x,y): return (x**2 + y**2) 123456789101112n = 256x = np.linspace(-3,3,n)y = np.linspace(-3,3,n)X,Y = np.meshgrid(x,y)plt.contourf(X,Y,f(X,Y),7,alpha=0.5,cmap=plt.cm.cool)C = plt.contour(X,Y,f(X,Y),7,colors='black')plt.clabel(C,inline=True,)plt.xticks(())plt.yticks(())plt.show() 1234a = np.random.normal(0,1,100).reshape((10,10))plt.imshow(a,interpolation='nearest')plt.colorbar(shrink=0.9)plt.show &lt;function matplotlib.pyplot.show(*args, **kw)&gt; 1from mpl_toolkits.mplot3d import Axes3D 12345678910fig = plt.figure()ax = Axes3D(fig)X = np.arange(-4,4,0.25)Y = np.arange(-4,4,0.25)X,Y = np.meshgrid(X,Y)R = np.sqrt(X**2+Y**2)ax.plot_surface(X,Y,R,cmap='rainbow')ax.contourf(X,Y,R,zdir='z',offset=0,cmap='rainbow')plt.show() 12345678910plt.figure()plt.subplot(2,3,4)plt.plot(np.arange(1,100))plt.subplot(2,3,5)plt.plot(np.arange(1,20)**2)plt.subplot(2,1,1)plt.plot(np.random.normal(0,1,100))plt.subplot(2,3,6)plt.plot(np.array(np.sin(np.arange(0,10,0.1))))plt.show() 1import matplotlib.gridspec as gridspec 1234567891011# plt.figure()# ax1 = plt.subplot2grid((3,3),(0,0),colspan=3,rowspan=1)# ax1.plot(np.random.normal(0,1,100))# ax1.set_title('time')# ax2 = plt.subplot2grid((3,3),(1,0),colspan=2,rowspan=1)# ax2 = plt.subplot2grid((3,3),(1,2),colspan=1,rowspan=2)# ax2 = plt.subplot2grid((3,3),(2,0),colspan=1,rowspan=1)# ax2 = plt.subplot2grid((3,3),(2,1),colspan=1,rowspan=1)# plt.show() 1234567# plt.figure()# gs = gridspec.GridSpec(3,3)# ax1 = plt.subplot(gs[0,:])# ax2 = plt.subplot(gs[1,:2])# ax3 = plt.subplot(gs[1:,2])# ax4 = plt.subplot(gs[-1,0])# ax5 = plt.subplot(gs[-1,1]) 123456789101112x = np.arange(0,50)y1 = np.random.normal(0,1,50)y2 = np.random.normal(1,2,50)fig,ax1 = plt.subplots()ax2 = ax1.twinx()ax1.plot(x,y1,'g-')ax2.plot(x,-y2,'b-')ax1.set_xlabel('X data')ax1.set_ylabel('Y1',color='g')ax2.set_ylabel('Y2',color='b')plt.show() 1from scipy.stats import t 12345x = np.arange(-5,5,0.1)plt.figure()plt.plot(x,t.pdf(x,2))plt.plot(x,t.pdf(x,10))plt.show() 12","link":"/2019/03/26/matplotlib/"},{"title":"我的第一个神经网络——手写数字识别","text":"可从该页面获得的MNIST手写数字数据库具有60,000个示例的训练集和10,000个示例的测试集。它是NIST提供的更大集合的子集。数字已经过尺寸标准化，并以固定尺寸的图像为中心。 手写数字数据集THE MNIST DATABASE 123456789101112131415161718192021222324from keras.datasets import mnistfrom keras.utils import np_utilsfrom keras.optimizers import Adamfrom keras.models import Sequentialfrom keras.layers import Dense, Activation# 清洗数据def load_data(): global x_train, x_test, y_train, y_test x_train = x_train.reshape(x_train.shape[0], 28*28) x_test = x_test.reshape(x_test.shape[0], 28*28) x_train = x_train.astype('float32') x_test = x_test.astype('float32') y_train = np_utils.to_categorical(y_train, 10) y_test = np_utils.to_categorical(y_test, 10) x_train = x_train/255 x_test = x_test/255 return (x_train, y_train),(x_test, y_test)# 导入数据(x_train, y_train),(x_test, y_test) = mnist.load_data()(x_train, y_train),(x_test, y_test) = load_data() 12345678# 模型搭建model = Sequential()model.add(Dense(input_dim=28*28, units=500))model.add(Activation('relu'))model.add(Dense(units=500))model.add(Activation('relu'))model.add(Dense(units=10))model.add(Activation('softmax')) 1234# 模型配置model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) 1234# 训练model.fit(x_train, y_train, epochs=20, batch_size=100) Instructions for updating: Use tf.cast instead. Epoch 1/20 60000/60000 [==============================] - 5s 89us/step - loss: 0.2038 - acc: 0.9390 ... 60000/60000 [==============================] - 5s 86us/step - loss: 0.0051 - acc: 0.9985 Epoch 20/20 60000/60000 [==============================] - 5s 84us/step - loss: 0.0095 - acc: 0.9973 &lt;keras.callbacks.History at 0x103d9b710&gt; 1234# 训练评分score = model.evaluate(x_train, y_train, batch_size=100)print('Total loss in training set:', score[0])print('Accuracy in training set:', score[1]) 60000/60000 [==============================] - 2s 26us/step Total loss in training set: 0.030331732142293125 Accuracy in training set: 0.9930500061313311 1234# 测试评分score = model.evaluate(x_test, y_test, batch_size=100)print('Total loss in testing set:', score[0])print('Accuracy in testing set:', score[1]) 10000/10000 [==============================] - 0s 27us/step Total loss in testing set: 0.13895348183807527 Accuracy in testing set: 0.9754000073671341","link":"/2019/04/02/MNIST/"},{"title":"留言板 📮","text":"欢迎留下评论～","link":"/2000/03/02/留言板/"},{"title":"一道排序题","text":"给定一组数，每次只能交换两个数字的位置，求全部排序完成的最小交换次数 12345678910111213141516def s(l): k = 0 for i in range(len(l)): li = l[i:] j = li.index(min(li)) if j == 0: continue l[i], l[i+j] = l[i+j], l[i] k += 1 return kl = [5,4,3,2,1]print(s(l))# &gt;&gt;&gt; 2","link":"/2019/03/30/一道排序题/"},{"title":"硬train的故事","text":"Thanks a lot, machine learning! Joel GrusFizz Buzz in Tensorflow","link":"/2019/03/30/video/"},{"title":"Numpy笔记📒","text":"NumPy(Numerical Python) 是 Python 语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。123456import numpy as npl = [[1,2,3],[2,3,4]]# 列表转化成矩阵array = np.array(l)print(array) [[1 2 3] [2 3 4]] numpy的几种属性123456# 维度print('number of dim:',array.ndim)# 行数和列数print('shape :',array.shape) # 元素个数print('size:',array.size) number of dim: 2 shape : (2, 3) size: 6 创建array12a = np.array([2,3,4],dtype = np.int)print(a,a.dtype) [2 3 4] int64 123# 创建全0数组a = np.zeros((3,4),dtype = np.int16)print(a) [[0 0 0 0] [0 0 0 0] [0 0 0 0]] 123# 创建全1数组a = np.ones((3,4),dtype = np.int64)print(a) [[1 1 1 1] [1 1 1 1] [1 1 1 1]] 123# 创建全空数组a = np.empty((3,4),dtype = np.float)print(a) [[-1.49166815e-154 -1.49166815e-154 4.27255699e+180 6.12033286e+257] [ 3.83819517e+151 9.77368093e+165 1.03927302e-042 5.24049485e+174] [ 4.27796595e-033 5.81088333e+294 -1.49166815e-154 8.38743761e-309]] 123# 创建连续数组a = np.arange(1,13).reshape(3,4)print(a) [[ 1 2 3 4] [ 5 6 7 8] [ 9 10 11 12]] 123# 生成线段a = np.linspace(1,10,6).reshape(2,3)print(a) [[ 1. 2.8 4.6] [ 6.4 8.2 10. ]] 基础运算基本运算1数组运算12a = np.array([10,20,30,40])b = np.arange(4) 1a-b array([10, 19, 28, 37]) 1b**2 array([0, 1, 4, 9]) 1np.sin(a) array([-0.54402111, 0.91294525, -0.98803162, 0.74511316]) 1b&lt;3 array([ True, True, True, False]) 矩阵运算1234a = np.array([[1,2],[3,4]])b = np.arange(4).reshape(2,2)print(a)print(b) [[1 2] [3 4]] [[0 1] [2 3]] 1234# 对应元素相乘print(a*b)# 矩阵乘法print(np.dot(a,b)) [[ 0 2] [ 6 12]] [[ 4 7] [ 8 15]] 123456a = np.random.random((2,4))print(a)# axis=1时每行分别计算，axis=0时每列分别计算print('sum =',np.sum(a,axis=1))print('min =',np.min(a,axis=0))print('max =',np.max(a)) [[0.31357817 0.09926399 0.57284534 0.9692283 ] [0.86206853 0.94729865 0.80886452 0.01849844]] sum = [1.9549158 2.63673014] min = [0.31357817 0.09926399 0.57284534 0.01849844] max = 0.9692282997410551 基本运算212A = np.arange(2,14).reshape(3,4)print(A) [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] 12# 最小（大）值索引A.argmax() 11 1234# 平均值print(A.mean())# 对列求平均print(A.mean(axis=0)) 7.5 [6. 7. 8. 9.] 12# 中位数np.median(A) 7.5 12# 累加print(np.cumsum(A)) [ 2 5 9 14 20 27 35 44 54 65 77 90] 12# 累差print(np.diff(A)) [[1 1 1] [1 1 1] [1 1 1]] 12# 排序print(np.sort(A)) [[ 2 3 4 5] [ 6 7 8 9] [10 11 12 13]] 12# 转置print(np.transpose(A)) [[ 2 6 10] [ 3 7 11] [ 4 8 12] [ 5 9 13]] 12# A*ATprint(np.dot(A,A.T)) [[ 54 110 166] [110 230 350] [166 350 534]] 12# 滤波print(np.clip(A,5,9)) [[5 5 5 5] [6 7 8 9] [9 9 9 9]] 12# 铺平print(A.flatten()) [ 2 3 4 5 6 7 8 9 10 11 12 13] numpy索引12A = np.arange(3,15).reshape(3,4)print(A) [[ 3 4 5 6] [ 7 8 9 10] [11 12 13 14]] 12A[1,1]#A[1][1] 8 array合并1234A = np.array([1,1,1])B = np.array([2,2,2])np.vstack((A,B)) array([[1, 1, 1], [2, 2, 2]]) 1np.hstack((A,B)) array([1, 1, 1, 2, 2, 2]) array分割12A = np.arange(12).reshape(3,4)print(A) [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 12345np.split(A,3)# 横向分割# np.vsplit(A,3)# 纵向分割# np.hsplit(A,2) [array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8, 9, 10, 11]])] 12# 不等量分割np.array_split(A,3,axis=1) [array([[0, 1], [4, 5], [8, 9]]), array([[ 2], [ 6], [10]]), array([[ 3], [ 7], [11]])] copy123a = np.array([1,2])# b.copy()没有关联性，为浅拷贝a is b.copy() False","link":"/2019/03/02/numpy/"},{"title":"Keras手把手教学之 💯","text":"Keras Sequential 顺序模型 模型搭建直接看例子点这里 👈🏻 Sequential 模型是层的线性堆栈 12345678910111213141516import kerasfrom keras.models import Sequentialfrom keras.layers import Dense, Activationmodel = Sequential()# Dense: fully connection(DNN密集型神经网络)model.add(Dense(input_dim=28*28,units=500))# activation function:# softplus, softsign, relu, tanh, hard_sigmoid, linearmodel.add(Activation('sigmoid'))# the second hidden layermodel.add(Dense(units=500))model.add(Activation('sigmoid'))model.add(Dense(units=10))model.add(Activation('softmax')) 模型配置在训练模型之前，您需要配置学习过程，这是通过 compile 方法完成的。它接收三个参数： 优化器 optimizer。它可以是现有优化器的字符串标识符，如 rmsprop 或 adagrad，也可以是 Optimizer 类的实例。 损失函数 loss，模型试图最小化的目标函数。它可以是现有损失函数的字符串标识符，如 categorical_crossentropy 或 mse，也可以是一个目标函数。 评估标准 metrics。对于任何分类问题，你都希望将其设置为 metrics = ['accuracy']。 123456789101112131415161718# 二分类问题model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])# 均方误差回归问题model.compile(optimizer='rmsprop', loss='mse')# 自定义评估标准函数import keras.backend as Kdef mean_pred(y_true, y_pred): return K.mean(y_pred)model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy', mean_pred]) 123456# step 3.1:# Configuration# optimizer = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam]model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) 模型训练Keras 模型在输入数据和标签的 Numpy 矩阵上进行训练。为了训练一个模型，你通常会使用 fit 函数。 1234567# 具有 10 个类的单输入模型（多分类分类）：# 生成虚拟数据import numpy as npdata = np.random.random((50000, 28*28))labels = np.random.randint(10, size=(50000, 1))# 将标签转换为分类的 one-hot 编码one_hot_labels = keras.utils.to_categorical(labels, num_classes=10) 1234567891011# step 3.2:# Find the optimal network parameters# do not really minimize total loss# betch_size:# --&gt; pick the 1st batch L' = L1 + L31 + ...# update parameters once# --&gt; pick the 2nd batch ...# all of the betch make one epoch: need 20 epoch# when beach_size = 1:# Stochastic gradient descentmodel.fit(data, one_hot_labels, epochs=20, batch_size=100) Epoch 1/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.3052 - acc: 0.1004 Epoch 2/20 50000/50000 [==============================] - 5s 92us/step - loss: 2.3046 - acc: 0.1019 Epoch 3/20 50000/50000 [==============================] - 4s 87us/step - loss: 2.3053 - acc: 0.0997 Epoch 4/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.3057 - acc: 0.1014 Epoch 5/20 50000/50000 [==============================] - 5s 92us/step - loss: 2.3057 - acc: 0.1027 Epoch 6/20 50000/50000 [==============================] - 5s 91us/step - loss: 2.3065 - acc: 0.1035 Epoch 7/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.3077 - acc: 0.1034 Epoch 8/20 50000/50000 [==============================] - 5s 94us/step - loss: 2.3081 - acc: 0.1039 Epoch 9/20 50000/50000 [==============================] - 4s 89us/step - loss: 2.3057 - acc: 0.1109 Epoch 10/20 50000/50000 [==============================] - 4s 87us/step - loss: 2.3012 - acc: 0.1190 Epoch 11/20 50000/50000 [==============================] - 4s 87us/step - loss: 2.2925 - acc: 0.1285 Epoch 12/20 50000/50000 [==============================] - 4s 88us/step - loss: 2.2836 - acc: 0.1375 Epoch 13/20 50000/50000 [==============================] - 4s 84us/step - loss: 2.2744 - acc: 0.1476 Epoch 14/20 50000/50000 [==============================] - 4s 86us/step - loss: 2.2675 - acc: 0.1526 Epoch 15/20 50000/50000 [==============================] - 4s 85us/step - loss: 2.2638 - acc: 0.1551 Epoch 16/20 50000/50000 [==============================] - 4s 80us/step - loss: 2.2556 - acc: 0.1602 Epoch 17/20 50000/50000 [==============================] - 4s 83us/step - loss: 2.2525 - acc: 0.1638 Epoch 18/20 50000/50000 [==============================] - 5s 94us/step - loss: 2.2534 - acc: 0.1617 Epoch 19/20 50000/50000 [==============================] - 5s 93us/step - loss: 2.2462 - acc: 0.1681 Epoch 20/20 50000/50000 [==============================] - 4s 83us/step - loss: 2.2403 - acc: 0.1694 &lt;keras.callbacks.History at 0x126444278&gt; Example基于多层感知器 (MLP) 的 softmax 多分类（分类器）： 123456789101112131415import kerasfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Activationfrom keras.optimizers import SGD# 生成虚拟数据import numpy as np# 1000行*20列 的区间在[0.,1.)的浮点数x_train = np.random.random((1000, 20))# 生成 1000行*1列 的区间在[0-10)的整数，随后转化为分类的 one-hot 编码（1000行*10列 的(0,1)矩阵）y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)# 100行*20列 的测试集x_test = np.random.random((100, 20))# 100行*10列 的测试标签y_test = keras.utils.to_categorical(np.random.randint(10, size=(100, 1)), num_classes=10) 123456789model = Sequential()# Dense(64) 是一个具有 64 个隐藏神经元的全连接层。# 在第一层必须指定所期望的输入数据尺寸：# 在这里，是一个 20 维的向量。model.add(Dense(units=64, activation='relu', input_dim=20))model.add(Dropout(rate=0.5))model.add(Dense(units=64, activation='relu'))model.add(Dropout(rate=0.5))model.add(Dense(units=10, activation='softmax')) 123456# 优化器sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)# 模型配置model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) 1234# 模型训练model.fit(x_train, y_train, epochs=20, batch_size=128) Epoch 1/20 1000/1000 [==============================] - 0s 402us/step - loss: 2.4034 - acc: 0.1010 Epoch 2/20 1000/1000 [==============================] - 0s 19us/step - loss: 2.3808 - acc: 0.0920 Epoch 3/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3396 - acc: 0.1120 Epoch 4/20 1000/1000 [==============================] - 0s 23us/step - loss: 2.3249 - acc: 0.1190 Epoch 5/20 1000/1000 [==============================] - 0s 20us/step - loss: 2.3356 - acc: 0.0980 Epoch 6/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3233 - acc: 0.1210 Epoch 7/20 1000/1000 [==============================] - 0s 18us/step - loss: 2.3070 - acc: 0.1080 Epoch 8/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3190 - acc: 0.1030 Epoch 9/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3112 - acc: 0.0900 Epoch 10/20 1000/1000 [==============================] - 0s 20us/step - loss: 2.3076 - acc: 0.0960 Epoch 11/20 1000/1000 [==============================] - 0s 15us/step - loss: 2.3067 - acc: 0.1060 Epoch 12/20 1000/1000 [==============================] - 0s 16us/step - loss: 2.3058 - acc: 0.1160 Epoch 13/20 1000/1000 [==============================] - 0s 16us/step - loss: 2.2988 - acc: 0.1180 Epoch 14/20 1000/1000 [==============================] - 0s 17us/step - loss: 2.3008 - acc: 0.1240 Epoch 15/20 1000/1000 [==============================] - 0s 20us/step - loss: 2.3036 - acc: 0.1090 Epoch 16/20 1000/1000 [==============================] - 0s 22us/step - loss: 2.3042 - acc: 0.1060 Epoch 17/20 1000/1000 [==============================] - 0s 18us/step - loss: 2.3016 - acc: 0.1070 Epoch 18/20 1000/1000 [==============================] - 0s 25us/step - loss: 2.3054 - acc: 0.1390 Epoch 19/20 1000/1000 [==============================] - 0s 39us/step - loss: 2.2960 - acc: 0.1200 Epoch 20/20 1000/1000 [==============================] - 0s 34us/step - loss: 2.3021 - acc: 0.0940 &lt;keras.callbacks.History at 0x1842de6b38&gt; 12# 模型评分score = model.evaluate(x_test, y_test, batch_size=128) 100/100 [==============================] - 0s 13us/step","link":"/2019/03/27/keras notebook/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"机器学习","slug":"机器学习","link":"/tags/机器学习/"},{"name":"算法","slug":"算法","link":"/tags/算法/"}],"categories":[{"name":"python","slug":"python","link":"/categories/python/"},{"name":"machine learning","slug":"machine-learning","link":"/categories/machine-learning/"},{"name":"算法","slug":"算法","link":"/categories/算法/"}]}